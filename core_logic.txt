Project Instruction: Multi-Model AI Response Fusion via Cerebras API

====================================

1. .env Configuration
---------------------
Store the following variables in your .env file:

CEREBRAS_API_KEY=apicsk-rtdjvfdm6rky3tym2w53xewhkp2jy8e846cf9e3hnyen2dw8
MAX_TOKENS=1024
STREAM=false

MODEL1=qwen-3-235b-a22b-instruct-2507
MODEL2=gpt-oss-120b
MODEL3=llama-4-maverick-17b-128e-instruct

JUDGE=qwen-3-235b-a22b-thinking-2507

CODE_X_KEY=your_custom_api_key_here

====================================

2. How to Invoke Cerebras API
------------------------------
Use the following CURL command (replace AI_MODEL with the actual model name):

curl --location 'https://api.cerebras.ai/v1/chat/completions' \\
--header 'Content-Type: application/json' \\
--header "Authorization: Bearer ${CEREBRAS_API_KEY}" \\
--data '{
  "model": "AI_MODEL",
  "stream": false,
  "max_tokens": 1024,
  "temperature": 1,
  "top_p": 1,
  "reasoning_effort": "medium",
  "messages": [
    {
      "role": "system",
      "content": ""
    },
    {
      "role": "user",
      "content": "YOUR_USER_INPUT_HERE"
    }
  ]
}'

====================================

3. Supported Models
-------------------

Main Models:
- qwen-3-235b-a22b-instruct-2507
- gpt-oss-120b
- llama-4-maverick-17b-128e-instruct

Judge Model:
- qwen-3-235b-a22b-thinking-2507

====================================

4. Core Logic & Flow
---------------------

Step 1: User sends a question via API request (with "code-x-key" header).
Step 2: The backend forwards the same input to all 3 models (MODEL1, MODEL2, MODEL3).
Step 3: The outputs from the 3 models are passed to the judge model (JUDGE).
Step 4: The judge synthesizes the final answer based on the 3 responses.
Step 5: The synthesized response is returned to the user.

====================================

5. Requirements
----------------

- Must expose an HTTP API endpoint (e.g., /ask)
- Access must require a valid "code-x-key" header (matched against .env value)
- All secrets and model settings are read from the .env file
- Fully Dockerized deployment

====================================

6. Output JSON Format (Example)
-------------------------------

{
  "input": "What is the future of quantum computing?",
  "models": {
    "MODEL1": "response text",
    "MODEL2": "response text",
    "MODEL3": "response text"
  },
  "judge": {
    "final_answer": "combined improved response",
    "reasoning": "Summarized based on overlap and clarity from the three."
  }
}

====================================

7. Dockerization Notes
-----------------------

- Use a Python 3.11-based image (e.g., python:3.11-slim)
- Install dependencies via requirements.txt
- Expose API using FastAPI + Uvicorn
- Do not hardcode secrets in code; read from environment
- Mount .env at runtime
- Use ENTRYPOINT to start Uvicorn server

====================================
